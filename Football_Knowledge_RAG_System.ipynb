{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phattaraphol/Football-Knowledge-RAG-System/blob/main/Football_Knowledge_RAG_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install all dependencies"
      ],
      "metadata": {
        "id": "4P8ytsQe_MEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neo4j"
      ],
      "metadata": {
        "id": "B4RsmAF0UYsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-neo4j"
      ],
      "metadata": {
        "id": "f4VYkx-mXMVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ถอนการติดตั้งเพื่อไม่ให้ cache ใน colab กับ langchain เวอร์ชั่นเก่าเอาไว้\n",
        "!pip uninstall -y langchain langchain-community langchain-core langchain-google-genai langchain-huggingface\n",
        "\n",
        "#ติดตั้งใหม่โดย \"ไม่ใช้ Cache\" (--no-cache-dir) และ \"ระบุเวอร์ชันล่าสุดที่เสถียร\"\n",
        "#จากนั้นตัว colab จะให้เรา restart run-time\n",
        "!pip install --no-cache-dir \\\n",
        "    langchain==0.3.7 \\\n",
        "    langchain-community==0.3.7 \\\n",
        "    langchain-core==0.3.19 \\\n",
        "    langchain-google-genai \\\n",
        "    langchain-huggingface \\\n",
        "    faiss-cpu\n"
      ],
      "metadata": {
        "id": "XFcYqTeYRr7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "print(f\"LangChain Version: {langchain.__version__}\") #ต้องขึ้น 0.3.7 ไม่งั้นจะ import RetrievalQA ไม่ได้\n",
        "\n",
        "#ลอง import\n",
        "from langchain.chains import RetrievalQA\n",
        "print(\"Import Success!\")"
      ],
      "metadata": {
        "id": "UqphyuQeQ8Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare data"
      ],
      "metadata": {
        "id": "u40yWsVe-WQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from neo4j import GraphDatabase\n",
        "\n",
        "uri = \"neo4j+s://3581f28e.databases.neo4j.io\"\n",
        "user = \"neo4j\"\n",
        "password = \"pOtJfLTvttU6Dl4Vu5AxnwHeuY-ZatBWdhP4A_9kBjo\"\n",
        "\n",
        "driver = GraphDatabase.driver(uri, auth=(user, password))\n"
      ],
      "metadata": {
        "id": "N0mViSYMUcWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"https://gist.githubusercontent.com/monkub003/1c6e1278930e65946e9b42ba869f453c/raw/64d6e21b974d5438c348dbf746b8b1a332e2f355/Football.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Snxi0H0qVc2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Push data into Neo4j (Already pushed, shouldn't be repeat)"
      ],
      "metadata": {
        "id": "kHzCDo2z4cBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def push_spo(tx, s, p, o):\n",
        "#     query = \"\"\"\n",
        "#     MERGE (sub:Entity {name: $s})\n",
        "#     MERGE (obj:Entity {name: $o})\n",
        "#     MERGE (sub)-[r:`%s`]->(obj)\n",
        "#     \"\"\" % p.replace(\" \", \"_\")\n",
        "#     tx.run(query, s=s, o=o)\n",
        "\n",
        "# with driver.session() as session:\n",
        "#     for index, row in df.iterrows():\n",
        "#         s = str(row[\"Subject\"])\n",
        "#         p = str(row[\"Predicate\"])\n",
        "#         o = str(row[\"Object\"])\n",
        "\n",
        "#         if pd.isna(s) or pd.isna(p) or pd.isna(o):\n",
        "#             continue\n",
        "\n",
        "#         session.execute_write(push_spo, s, p, o)\n",
        "\n",
        "# print(\"Data pushed to Neo4j successfully!\")\n"
      ],
      "metadata": {
        "id": "NuwCx-D9UtFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with driver.session() as session:\n",
        "    result = session.run(\"MATCH (n)-[r]->(m) RETURN n,r,m LIMIT 25\")\n",
        "    for record in result:\n",
        "        print(record)\n"
      ],
      "metadata": {
        "id": "Nt-9Y7xvbWOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG"
      ],
      "metadata": {
        "id": "Yc1B8PZFWKcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from neo4j import GraphDatabase\n",
        "import difflib\n",
        "\n",
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "id": "scgBgiyvEdMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "google_API_key = \"API_KEY\""
      ],
      "metadata": {
        "id": "LKNCLIgTIpSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "สำหรับเช็ค Available model"
      ],
      "metadata": {
        "id": "oHkt5p_SWAjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key=google_API_key)\n",
        "\n",
        "print(\"Available Models:\")\n",
        "for m in genai.list_models():\n",
        "    if 'generateContent' in m.supported_generation_methods:\n",
        "        print(m.name)"
      ],
      "metadata": {
        "id": "u0uxZjRrUcUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ตั้งค่า LLM และ Prompt Template (ตามข้อ 4.3)"
      ],
      "metadata": {
        "id": "N8PgJgz5auX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ตั้งค่า Environment Variable\n",
        "os.environ[\"GOOGLE_API_KEY\"] = google_API_key\n",
        "\n",
        "#สร้างโมเดล LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "\n",
        "    # แก้ไขตรงนี้: เรียกใช้ผ่าน os.environ โดยอ้างอิง \"ชื่อตัวแปร\" ที่เราตั้งไว้ข้างบน\n",
        "    google_api_key=os.environ[\"GOOGLE_API_KEY\"]\n",
        ")\n",
        "\n",
        "#สร้าง Prompt Template\n",
        "rag_prompt_template = \"\"\"\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Instruction:\n",
        "1. Answer in the Thai language.\n",
        "2. Do not use outside knowledge or hallucinate information.\n",
        "3. If the context is raw JSON data. Your task is to interpret this data and convert it into a natural, easy-to-read response.\n",
        "4. Keep the answer professional and concise.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=rag_prompt_template,\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")"
      ],
      "metadata": {
        "id": "WnT3ubrfVk74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "เตรียมข้อมูลสำหรับ Vector Search"
      ],
      "metadata": {
        "id": "fasbdM9bamYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#แปลง DataFrame แต่ละแถวให้เป็นข้อความ 1 ประโยค\n",
        "#ตัวอย่าง: \"Manchester United playsIn Premier League\"\n",
        "documents = [\n",
        "    f\"{row['Subject']} {row['Predicate']} {row['Object']}\"\n",
        "    for index, row in df.iterrows()\n",
        "]\n",
        "\n",
        "#สร้าง Embedding Model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "kYJN15bLJp02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector DB only (FAISS) (ข้อ 4.2.1)"
      ],
      "metadata": {
        "id": "Lt7jbNhCWm_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "vectorstore_faiss = FAISS.from_texts(documents, embeddings)\n",
        "\n",
        "#สร้าง Retriever (กำหนด k=5 ตามที่คุณต้องการก่อนหน้า)\n",
        "retriever_faiss = vectorstore_faiss.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 5}\n",
        ")\n",
        "\n",
        "#สร้าง Chain พร้อมสั่งให้คืนค่า Source Documents\n",
        "rag_1_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever_faiss,\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        "    return_source_documents=True\n",
        ")\n",
        "#ทดสอบและแสดงผล\n",
        "query = \"Which team plays in Premier League?\"\n",
        "response = rag_1_chain.invoke({\"query\": query})\n",
        "\n",
        "print(\"\\n=== ข้อมูล Context ที่ดึงมาใช้ (Top-5) ===\")\n",
        "for i, doc in enumerate(response['source_documents']):\n",
        "    print(f\"Docs #{i+1}: {doc.page_content}\")\n",
        "\n",
        "print(f\"\\nFinal Answer: {response['result']}\")"
      ],
      "metadata": {
        "id": "MRvkihO58KFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph DB (Neo4j Relationships) (ข้อ 4.2.2)"
      ],
      "metadata": {
        "id": "rkH7muqNWoIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_neo4j import GraphCypherQAChain\n",
        "from langchain_neo4j import Neo4jVector\n",
        "from langchain_neo4j import Neo4jGraph\n",
        "\n",
        "#เชื่อมต่อ Graph สำหรับ LangChain\n",
        "graph = Neo4jGraph(url=uri, username=user, password=password)\n",
        "\n",
        "#สร้าง Chain แบบ Graph Text-to-Cypher\n",
        "rag_2_chain = GraphCypherQAChain.from_llm(\n",
        "    llm = llm,\n",
        "    graph = graph,\n",
        "    verbose = True,            # ให้โชว์ Cypher ที่มันเขียนให้เราดู\n",
        "    qa_prompt = prompt,        # ใช้ Prompt ภาษาไทยที่เราเตรียมไว้\n",
        "    allow_dangerous_requests = True,\n",
        "    top_k = 5\n",
        ")\n",
        "\n",
        "#ทดสอบ ถามหาความสัมพันธ์ที่ต้อง Join ข้อมูล\n",
        "print(\"=== RAG 2 (Graph Pattern Match) ===\")\n",
        "query_rag2 = \"Liverpool plays in which league and who is their rival?\"\n",
        "response_rag2 = rag_2_chain.invoke({\"query\": query_rag2})\n",
        "\n",
        "print(f\"\\nFinal Answer: {response_rag2['result']}\")"
      ],
      "metadata": {
        "id": "8rTRvywc0Qau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Knowledge Graph (Node + Relationship + Attribute) (ข้อ 4.2.3)"
      ],
      "metadata": {
        "id": "-xujF356adwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "def run_hybrid_rag(user_query):\n",
        "    print(f\"1️ [User Query]: {user_query}\")\n",
        "\n",
        "    #Vector Grounding หา Node ตั้งต้น\n",
        "    #ใช้ FAISS ที่เราทำไว้ใน RAG 1 มาช่วยหา \"ตัวตน\" ที่ใกล้เคียงที่สุด\n",
        "    docs = vectorstore_faiss.similarity_search(user_query, k = 5)\n",
        "\n",
        "    retrieved_text = docs[0].page_content\n",
        "    print(f\"2️ [Vector Found]: {retrieved_text}\")\n",
        "\n",
        "    #สร้าง Prompt สั้นๆ ให้ LLM ช่วยดึงชื่อ Subject ออกมา\n",
        "    extraction_prompt = PromptTemplate(\n",
        "        input_variables=[\"text\"],\n",
        "        template=\"Extract the main entity name (Subject) from this text: '{text}'. Return ONLY the name.\"\n",
        "    )\n",
        "    extractor_chain = extraction_prompt | llm | StrOutputParser()\n",
        "    entity_name = extractor_chain.invoke({\"text\": retrieved_text}).strip()\n",
        "    print(f\"3️ [Entity Grounded]: {entity_name}\")\n",
        "\n",
        "    #Graph Traversal\n",
        "    #เอาชื่อที่ถูกต้อง entity_name ไปรัน Cypher เพื่อดึงข้อมูลรอบตัว 1-hop\n",
        "    cypher_query = \"\"\"\n",
        "    MATCH (n:Entity {name: $name})-[r]-(m)\n",
        "    RETURN n.name AS Subject, type(r) AS Relationship, m.name AS Object\n",
        "    \"\"\"\n",
        "\n",
        "    #รันคำสั่งใน Neo4j\n",
        "    graph_data = graph.query(cypher_query, params={\"name\": entity_name})\n",
        "    print(f\"4️ [Graph Traversal]: Found {len(graph_data)} relationships\")\n",
        "\n",
        "    #Generate Answer\n",
        "    #เอาผลลัพธ์จาก Graph ไปให้ LLM ตอบคำถาม\n",
        "    context_str = str(graph_data)\n",
        "\n",
        "    final_prompt = f\"\"\"\n",
        "    Context from Graph Database:\n",
        "    {context_str}\n",
        "\n",
        "    User Question: {user_query}\n",
        "\n",
        "    Answer in Thai (3 sentences):\n",
        "    \"\"\"\n",
        "\n",
        "    final_answer = llm.invoke(final_prompt).content\n",
        "    return final_answer\n",
        "\n",
        "#ทดสอบ RAG 3 Hybrid\n",
        "print(\"\\n=== RAG 3 (Hybrid: Vector Grounding + Graph Traversal) ===\")\n",
        "#ลองแกล้งพิมพ์ผิด หรือถามกว้างๆ ให้ Vector ช่วยจับ\n",
        "result = run_hybrid_rag(\"Tell me about Man U rivals\")\n",
        "print(f\"\\n Final Answer: {result}\")"
      ],
      "metadata": {
        "id": "bKYs8jkegc7D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}